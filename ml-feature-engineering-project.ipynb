{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc61b84",
   "metadata": {},
   "source": [
    "# CS 421 PROJECT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5b0da-1a70-409b-8117-4e831ff7da13",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2f77c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = np.load(\"data/regression_labelled.npz\")\n",
    "X     = data[\"X\"]\n",
    "y     = data[\"yy\"]\n",
    "y_cat = data[\"yy_cat\"]\n",
    "\n",
    "# Load dataframes\n",
    "X     = pd.DataFrame(X, columns=[\"user\", \"item\", \"rating\"])\n",
    "y     = pd.DataFrame(y, columns=[\"user\", \"label\"])\n",
    "y_cat = pd.DataFrame(y_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
    "\n",
    "# Parse to correct types\n",
    "y     = y.astype({\"user\": int, \"label\": float})\n",
    "y_cat = y_cat.astype({\"user\": int, \"label\": float, \"anomtype\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10da70-7e1f-416c-9ce2-f7cf4c78d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX    = np.load(\"data/regression_unlabelled.npz\")['X']\n",
    "XX    = pd.DataFrame(XX, columns=[\"user\", \"item\", \"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2f36d",
   "metadata": {},
   "source": [
    "# Feature Engineering  \n",
    "To estimate the noise level $p$ for each user, we extract user-level statistical features that capture consistency, randomness, and alignment with item averages in each user’s rating behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Intuition for Noise Level Prediction**\n",
    "\n",
    "- **Mean Rating** – Represents a user’s overall rating tendency.  \n",
    "  High-noise users often have averages close to the dataset’s midpoint (≈ 3) due to random rating behavior.  \n",
    "\n",
    "- **Standard Deviation of Ratings** – Captures the variability in a user’s ratings.  \n",
    "  A higher standard deviation generally indicates greater randomness and, therefore, higher noise.  \n",
    "\n",
    "- **Rating Range** – Reflects how widely a user utilizes the rating scale.  \n",
    "  Noisy users tend to rate across the full 1–5 range.  \n",
    "\n",
    "- **Number of Ratings** – Indicates how many items a user has rated.  \n",
    "  The noise generation process may influence this count, providing additional clues about the user’s noise level.  \n",
    "\n",
    "- **Median Rating** – Provides a robust measure of central tendency.  \n",
    "  Unlike the mean, it is less affected by extreme ratings.\n",
    "\n",
    "- **Median Absolute Deviation of Ratings** – Measures the spread of a user's ratings around their central tendency.\n",
    "  Low noise users should have more consistent biases to lower (1/2) and upper (4/5) ends while high noise users' ratings are more dispersed.\n",
    "\n",
    "- **Mean Item Popularity** – Represents the overall popularity of items a user rates.\n",
    "  Helps differentiate between users who seek out niche content compared to mainstream content\n",
    "\n",
    "- **Skewness of Ratings** – Measures asymmetry in the rating distribution.  \n",
    "  Helps identify users whose ratings are biased toward low or high values.\n",
    "\n",
    "- **Kurtosis of Ratings** – Measures the peakedness of the rating distribution.  \n",
    "  High kurtosis may indicate that ratings cluster around certain values.\n",
    "\n",
    "- **Kurtosis Ratio of Ratings** – Measures the ratio of peakedness of the rating distribution to the variability in a user's ratings.  \n",
    "  Helps with noise that avoid the midpoint (3) in ratings.\n",
    "\n",
    "- **Rating Entropy** – Captures the randomness or unpredictability in a user’s ratings.  \n",
    "  Higher entropy generally indicates higher noise.\n",
    "\n",
    "- **Item Average Entropy** – Captures the randomness or unpredictability in what items a user rates.  \n",
    "  Higher entropy generally indicates higher noise.\n",
    "\n",
    "- **Proportion of Extreme Ratings (1★ or 5★)** – Fraction of very low or very high ratings.  \n",
    "  High-noise users often have more evenly spread ratings, while consistent users may avoid extremes.\n",
    "\n",
    "- **Proportion of Extreme Disagreement** – Fraction of very low or very high ratings given to items that are generally not considered as such.  \n",
    "  High-noise users would not care, while consistent users would join the majority.\n",
    "\n",
    "- **Proportion of Unique Ratings (1★ or 5★)** – Fraction of the ratings a user has given out of the total (5).  \n",
    "  High-noise users would use more ratings, while consistent users may have a bias towards one end.\n",
    "\n",
    "- **Fraction of Consecutive Repeated Ratings** – Measures how often a user gives the same rating consecutively.  \n",
    "  Can indicate structured versus random behavior.\n",
    "\n",
    "- **Mean Deviation from Item Average** – Average difference between a user’s rating and the corresponding item’s mean rating.  \n",
    "  Noisy users tend to have ratings closer to 0 deviation, while structured users may systematically deviate.\n",
    "\n",
    "- **Standard Deviation of Deviations** – Measures variability in how a user deviates from item averages.  \n",
    "  Higher values indicate less predictable alignment with item norms.\n",
    "\n",
    "- **Average Absolute Deviation from Item Mean** – Average of the absolute deviations, providing a robust measure of alignment with typical item ratings.  \n",
    "  Captures overall disagreement with item norms, which can help identify noisy behavior.\n",
    "\n",
    "- **Median Absolute Deviation from Item Mean** – Median of the absolute deviations, providing a robust measure of alignment with typical item ratings.  \n",
    "  Captures overall consistency of disagreement with item norms, which can help identify noisy behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14ccea9-e508-4106-90eb-2ff8ac8dcd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>mean_rating</th>\n",
       "      <th>std_rating</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>median_rating</th>\n",
       "      <th>mad_rating</th>\n",
       "      <th>mean_item_popularity</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>prop_3</th>\n",
       "      <th>prop_5</th>\n",
       "      <th>prop_extreme_disagree</th>\n",
       "      <th>prop_match_item_mean</th>\n",
       "      <th>mean_dev</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>abs_mean_dev</th>\n",
       "      <th>med_abs_dev_item</th>\n",
       "      <th>cv_rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2700</td>\n",
       "      <td>2.304615</td>\n",
       "      <td>1.246340</td>\n",
       "      <td>325</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.780305</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>-0.807643</td>\n",
       "      <td>1.452700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.021538</td>\n",
       "      <td>0.236923</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>-0.900648</td>\n",
       "      <td>1.181644</td>\n",
       "      <td>1.231052</td>\n",
       "      <td>1.067416</td>\n",
       "      <td>0.540802</td>\n",
       "      <td>0.496169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2701</td>\n",
       "      <td>3.626898</td>\n",
       "      <td>0.522439</td>\n",
       "      <td>461</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.656944</td>\n",
       "      <td>-0.566175</td>\n",
       "      <td>-0.658135</td>\n",
       "      <td>0.754231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360087</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.427332</td>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.613991</td>\n",
       "      <td>0.577960</td>\n",
       "      <td>0.601093</td>\n",
       "      <td>0.144046</td>\n",
       "      <td>0.348425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2702</td>\n",
       "      <td>3.339286</td>\n",
       "      <td>1.165375</td>\n",
       "      <td>224</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.838453</td>\n",
       "      <td>-0.886810</td>\n",
       "      <td>0.813582</td>\n",
       "      <td>1.374229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.129464</td>\n",
       "      <td>0.169643</td>\n",
       "      <td>0.290179</td>\n",
       "      <td>0.236531</td>\n",
       "      <td>1.127293</td>\n",
       "      <td>0.919958</td>\n",
       "      <td>0.771128</td>\n",
       "      <td>0.348989</td>\n",
       "      <td>0.111702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2703</td>\n",
       "      <td>3.619658</td>\n",
       "      <td>1.119275</td>\n",
       "      <td>234</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.781465</td>\n",
       "      <td>-1.022916</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>1.320352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132479</td>\n",
       "      <td>0.170940</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.188034</td>\n",
       "      <td>0.358184</td>\n",
       "      <td>1.132465</td>\n",
       "      <td>1.011778</td>\n",
       "      <td>0.848164</td>\n",
       "      <td>0.309221</td>\n",
       "      <td>0.343628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2704</td>\n",
       "      <td>2.572414</td>\n",
       "      <td>1.137116</td>\n",
       "      <td>435</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.742189</td>\n",
       "      <td>-0.103944</td>\n",
       "      <td>-0.865041</td>\n",
       "      <td>1.440866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291954</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.291954</td>\n",
       "      <td>-0.580245</td>\n",
       "      <td>1.140364</td>\n",
       "      <td>1.045617</td>\n",
       "      <td>0.943522</td>\n",
       "      <td>0.442042</td>\n",
       "      <td>0.625995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  mean_rating  std_rating  num_ratings  median_rating  mad_rating  \\\n",
       "0  2700     2.304615    1.246340          325            2.0         1.0   \n",
       "1  2701     3.626898    0.522439          461            4.0         0.0   \n",
       "2  2702     3.339286    1.165375          224            3.5         0.5   \n",
       "3  2703     3.619658    1.119275          234            4.0         0.0   \n",
       "4  2704     2.572414    1.137116          435            3.0         1.0   \n",
       "\n",
       "   mean_item_popularity  skewness  kurtosis  rating_entropy  ...    prop_3  \\\n",
       "0              5.780305  0.010275 -0.807643        1.452700  ...  0.240000   \n",
       "1              5.656944 -0.566175 -0.658135        0.754231  ...  0.360087   \n",
       "2              5.838453 -0.886810  0.813582        1.374229  ...  0.321429   \n",
       "3              5.781465 -1.022916  0.335700        1.320352  ...  0.132479   \n",
       "4              5.742189 -0.103944 -0.865041        1.440866  ...  0.291954   \n",
       "\n",
       "     prop_5  prop_extreme_disagree  prop_match_item_mean  mean_dev   std_dev  \\\n",
       "0  0.021538               0.236923              0.240000 -0.900648  1.181644   \n",
       "1  0.008677               0.008677              0.427332  0.345794  0.613991   \n",
       "2  0.129464               0.169643              0.290179  0.236531  1.127293   \n",
       "3  0.170940               0.256410              0.188034  0.358184  1.132465   \n",
       "4  0.018391               0.206897              0.291954 -0.580245  1.140364   \n",
       "\n",
       "   abs_mean_dev  med_abs_dev_item  cv_rating     label  \n",
       "0      1.231052          1.067416   0.540802  0.496169  \n",
       "1      0.577960          0.601093   0.144046  0.348425  \n",
       "2      0.919958          0.771128   0.348989  0.111702  \n",
       "3      1.011778          0.848164   0.309221  0.343628  \n",
       "4      1.045617          0.943522   0.442042  0.625995  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "# Function to compute per-user features including deviation from item mean\n",
    "def extract_user_features(df):\n",
    "    # Compute global item average\n",
    "    item_mean = df.groupby(\"item\")[\"rating\"].mean().rename(\"global_item_mean\")\n",
    "    item_popularity = df.groupby(\"item\").size().rename(\"popularity\")\n",
    "\n",
    "    df = df.merge(item_mean, on=\"item\", how=\"left\")\n",
    "    df = df.merge(item_popularity, on=\"item\", how=\"left\")\n",
    "    \n",
    "    # Compute deviation from item mean for each rating\n",
    "    df[\"dev_from_item_mean\"] = df[\"rating\"] - df[\"global_item_mean\"]\n",
    "    df[\"item_mean_rounded\"] = df[\"global_item_mean\"].round().astype(int)\n",
    "    df[\"match_item_mean\"] = (df[\"rating\"] == df[\"item_mean_rounded\"]).astype(int)\n",
    "    df[\"log_popularity\"] = np.log(df[\"popularity\"] + 1)\n",
    "\n",
    "    df = df.drop(columns=[\"global_item_mean\", \"popularity\"])\n",
    "    \n",
    "    features_list = []\n",
    "\n",
    "    for user_id, group in df.groupby(\"user\"):\n",
    "        ratings = group[\"rating\"].values\n",
    "        devs = group[\"dev_from_item_mean\"].values\n",
    "        kurt_val = kurtosis(ratings)\n",
    "        \n",
    "        # Basic statistics\n",
    "        mean_rating = ratings.mean()\n",
    "        std_rating = ratings.std()\n",
    "        min_rating = ratings.min()\n",
    "        max_rating = ratings.max()\n",
    "        rating_range = max_rating - min_rating\n",
    "        num_ratings = len(ratings)\n",
    "        median_rating = np.median(ratings)\n",
    "\n",
    "        # 1. Calculate median of absolute deviations\n",
    "        user_median = np.median(ratings)\n",
    "        abs_dev_from_median = np.abs(ratings - user_median)\n",
    "        mad_rating = np.median(abs_dev_from_median)\n",
    "\n",
    "        if std_rating == 0:\n",
    "            # If spread is 0, the distribution is perfectly peaked.\n",
    "            kurtosis_ratio = 100.0 \n",
    "        else:\n",
    "            kurtosis_ratio = kurt_val / std_rating\n",
    "\n",
    "        mean_item_popularity = group[\"log_popularity\"].mean()\n",
    "        \n",
    "        # Advanced statistics\n",
    "        skewness = skew(ratings)\n",
    "        \n",
    "        # Entropy of rating distribution\n",
    "        counts = np.bincount(ratings, minlength=6)[1:]  # ratings 1-5\n",
    "        prob = counts / counts.sum()\n",
    "        rating_entropy = entropy(prob)\n",
    "\n",
    "        # Entropy of item rating distribution\n",
    "        item_avg_ratings = group[\"item_mean_rounded\"].values\n",
    "        item_avg_counts = np.bincount(item_avg_ratings, minlength=6)[1:] \n",
    "        item_avg_prob = item_avg_counts / item_avg_counts.sum()\n",
    "        item_avg_entropy = entropy(item_avg_prob)\n",
    "        \n",
    "        # Proportion of extreme ratings\n",
    "        prop_1 = np.sum(ratings == 1) / num_ratings\n",
    "        #prop_2 = np.sum(ratings == 2) / num_ratings\n",
    "        prop_3 = np.sum(ratings == 3) / num_ratings\n",
    "        prop_4 = np.sum(ratings == 4) / num_ratings\n",
    "        prop_5 = np.sum(ratings == 5) / num_ratings\n",
    "\n",
    "        # Proportion of unique ratings used\n",
    "        prop_unique_ratings = len(np.unique(ratings)) / num_ratings\n",
    "\n",
    "        # Proportion of disagreement, rated extremely when item is not\n",
    "        is_extreme_rating = (group[\"rating\"] == 1) | (group[\"rating\"] == 5)\n",
    "        is_not_extreme_item = (group[\"item_mean_rounded\"] > 1) & (group[\"item_mean_rounded\"] < 5)\n",
    "        disagreement_count = np.sum(is_extreme_rating & is_not_extreme_item)\n",
    "        prop_extreme_disagree = disagreement_count / num_ratings\n",
    "\n",
    "        # Proportion of ratings that match rounded item mean\n",
    "        prop_match_item_mean = group[\"match_item_mean\"].mean()\n",
    "        \n",
    "        # Fraction of repeated consecutive ratings\n",
    "        repeats = np.sum(ratings[1:] == ratings[:-1]) / (num_ratings - 1) if num_ratings > 1 else 0\n",
    "        \n",
    "        abs_devs = np.abs(devs)\n",
    "        mean_dev = devs.mean()\n",
    "        std_dev = devs.std()\n",
    "        abs_mean_dev = np.abs(devs).mean()\n",
    "        med_abs_dev_item = np.median(abs_devs)\n",
    "        \n",
    "        features_list.append({\n",
    "            \"user\": user_id,\n",
    "            \"mean_rating\": mean_rating,\n",
    "            \"std_rating\": std_rating,\n",
    "            #\"min_rating\": min_rating,\n",
    "            #\"max_rating\": max_rating,\n",
    "            #\"rating_range\": rating_range,\n",
    "            \"num_ratings\": num_ratings,\n",
    "            \"median_rating\": median_rating,\n",
    "            \"mad_rating\": mad_rating,\n",
    "            \"mean_item_popularity\": mean_item_popularity,\n",
    "            \"skewness\": skewness,\n",
    "            \"kurtosis\": kurt_val,\n",
    "            #\"kurtosis_ratio\": kurtosis_ratio,\n",
    "            \"rating_entropy\": rating_entropy,\n",
    "            \"item_avg_entropy\": item_avg_entropy,\n",
    "            \"prop_1\": prop_1,\n",
    "            \"prop_3\": prop_3,\n",
    "            \"prop_5\": prop_5,\n",
    "            \"prop_extreme_disagree\": prop_extreme_disagree,\n",
    "            #\"prop_unique_ratings\": prop_unique_ratings,\n",
    "            \"prop_match_item_mean\": prop_match_item_mean,\n",
    "            #\"repeat_fraction\": repeats,\n",
    "            \"mean_dev\": mean_dev,\n",
    "            \"std_dev\": std_dev,\n",
    "            \"abs_mean_dev\": abs_mean_dev,\n",
    "            \"med_abs_dev_item\": med_abs_dev_item,\n",
    "           \"cv_rating\": std_rating / mean_rating if mean_rating != 0 else 0,\n",
    "\n",
    "        })\n",
    "    \n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    return features_df\n",
    "\n",
    "# Apply to train and test datasets\n",
    "X_feat = extract_user_features(X)\n",
    "XX_feat = extract_user_features(XX)\n",
    "\n",
    "# Merge with labels for training\n",
    "train_df = pd.merge(X_feat, y, on=\"user\")\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c534c",
   "metadata": {},
   "source": [
    "# Task 1: Train Regression Model\n",
    "\n",
    "##### We first train on **Linear Regression** and **Polynomial Regression** and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d9df80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Comparison:\n",
      "                           Model       MAE\n",
      "0  Polynomial Regression (deg=2)  0.071884\n",
      "1              Linear Regression  0.108613\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# --- Prepare features and target ---\n",
    "feature_cols = [c for c in X_feat.columns if c != \"user\"]\n",
    "X_features = X_feat[feature_cols]\n",
    "y_labels = y[\"label\"]\n",
    "\n",
    "# --- Train-validation split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_features, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Scale features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# --- Linear Regression ---\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lin = np.clip(lin_reg.predict(X_val_scaled), 0, 1)  # clip to [0,1]\n",
    "mae_lin = mean_absolute_error(y_val, y_pred_lin)\n",
    "\n",
    "# --- Polynomial Regression (Degree 2) ---\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train)\n",
    "y_pred_poly = np.clip(poly_reg.predict(X_val_poly), 0, 1)  # clip to [0,1]\n",
    "mae_poly = mean_absolute_error(y_val, y_pred_poly)\n",
    "\n",
    "# --- Compare Results ---\n",
    "results_base = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Polynomial Regression (deg=2)\"],\n",
    "    \"MAE\": [mae_lin, mae_poly]\n",
    "}).sort_values(by=\"MAE\", ascending=True, ignore_index=True)\n",
    "\n",
    "print(\"Baseline Model Comparison:\")\n",
    "print(results_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8979e74",
   "metadata": {},
   "source": [
    "#### Choosing a Regularizer\n",
    "The polynomial regression (deg=2) model showed improved performance over the simple linear regression baseline, indicating that nonlinear relationships exist in the data.  \n",
    "To further enhance generalization and reduce potential overfitting from the polynomial terms, we now apply **regularized polynomial models** — Ridge, Lasso, and ElasticNet — which introduce penalty terms to balance model complexity and predictive accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c996f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Polynomial Model Comparison:\n",
      "                   Model       MAE\n",
      "0  Polynomial ElasticNet  0.064539\n",
      "1       Polynomial Ridge  0.070188\n",
      "2       Polynomial Lasso  0.070390\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Polynomial Ridge Regression ---\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train_poly, y_train)\n",
    "y_pred_ridge = np.clip(ridge.predict(X_val_poly), 0, 1)  # clip to [0,1]\n",
    "mae_ridge = mean_absolute_error(y_val, y_pred_ridge)\n",
    "\n",
    "# --- Polynomial Lasso Regression ---\n",
    "lasso = Lasso(alpha=0.001, max_iter=10000)\n",
    "lasso.fit(X_train_poly, y_train)\n",
    "y_pred_lasso = np.clip(lasso.predict(X_val_poly), 0, 1)\n",
    "mae_lasso = mean_absolute_error(y_val, y_pred_lasso)\n",
    "\n",
    "# --- Polynomial ElasticNet Regression ---\n",
    "elastic = ElasticNet(alpha=0.001, l1_ratio=0.255, max_iter=10000)\n",
    "elastic.fit(X_train_poly, y_train)\n",
    "y_pred_elastic = np.clip(elastic.predict(X_val_poly), 0, 1)\n",
    "mae_elastic = mean_absolute_error(y_val, y_pred_elastic)\n",
    "\n",
    "# --- Compare Results ---\n",
    "results_reg = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Polynomial Ridge\",\n",
    "        \"Polynomial Lasso\",\n",
    "        \"Polynomial ElasticNet\"\n",
    "    ],\n",
    "    \"MAE\": [mae_ridge, mae_lasso, mae_elastic]\n",
    "}).sort_values(by=\"MAE\", ascending=True, ignore_index=True)\n",
    "\n",
    "print(\"Regularized Polynomial Model Comparison:\")\n",
    "print(results_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776029a",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "After comparing several regression approaches for predicting user noise level \\(p\\):\n",
    "\n",
    "- **Linear Regression** provided a simple baseline but struggled with nonlinear interactions (MAE ≈ 0.1145).  \n",
    "- **Polynomial Regression (degree 2)** captured non-linearities and improved performance (MAE ≈ 0.0948).  \n",
    "- **Regularized Polynomial Models** (Ridge, Lasso, ElasticNet) were tested to control overfitting. Among these, **Polynomial ElasticNet Regression** achieved the **best performance** with **MAE ≈ 0.0782**, demonstrating that combining polynomial features with L1 and L2 regularization effectively balances complexity and generalization.\n",
    "\n",
    "Thus, **Polynomial ElasticNet (degree 2)** is the recommended model for predicting user noise levels in this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932fe6c",
   "metadata": {},
   "source": [
    "# Task 2: Semi-Supervised Anomaly Type Prediction\n",
    "\n",
    "We predict the anomaly type $X$  using a semi-supervised approach:\n",
    "\n",
    "1. **Feature Preparation** – Use the same user-level statistical features as for regression.\n",
    "2. **Standardization** – Scale features to zero mean and unit variance for effective clustering.\n",
    "3. **Unsupervised Clustering** – Apply K-Means with 3 clusters to group users based on similarity in their feature profiles (distance in feature space).\n",
    "4. **Cluster-to-Class Mapping** – Use the small labeled set (`y_cat`) to assign cluster labels to actual anomaly types.\n",
    "5. **Prediction for All Users** – Assign predicted classes to all users, handling clusters without labeled users using nearest cluster centers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0867c2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to week1234_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "# --- Prepare features ---\n",
    "feature_cols = [c for c in X_feat.columns if c != \"user\"]\n",
    "X_train_features = X_feat[feature_cols]\n",
    "X_train_users = X_feat[\"user\"]\n",
    "\n",
    "# Optional: polynomial features (degree 2)\n",
    "poly_degree = 2\n",
    "poly = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_features)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_poly)\n",
    "\n",
    "# --- Step 1: Fit Gaussian Mixture Model ---\n",
    "n_clusters = 3\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "cluster_labels = gmm.fit_predict(X_train_scaled)\n",
    "\n",
    "# Map clusters to anomaly types using labeled users\n",
    "cluster_df = pd.DataFrame({\n",
    "    \"user\": X_train_users,\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n",
    "cluster_to_class = {}\n",
    "for cluster in np.unique(cluster_labels):\n",
    "    users_in_cluster = cluster_df[cluster_df[\"cluster\"] == cluster][\"user\"]\n",
    "    labeled_in_cluster = y_cat[y_cat[\"user\"].isin(users_in_cluster)]\n",
    "    \n",
    "    if not labeled_in_cluster.empty:\n",
    "        counts = labeled_in_cluster[\"anomtype\"].value_counts()\n",
    "        max_count = counts.max()\n",
    "        candidates = counts[counts == max_count].index.tolist()\n",
    "        cluster_to_class[cluster] = min(candidates)  # tie-breaker\n",
    "    else:\n",
    "        cluster_to_class[cluster] = -1\n",
    "\n",
    "cluster_df[\"predicted_anomtype\"] = cluster_df[\"cluster\"].map(cluster_to_class)\n",
    "\n",
    "# --- Step 2: Label Spreading for semi-supervised refinement ---\n",
    "# Prepare labels: -1 for unlabeled users\n",
    "labels = np.full(X_train_scaled.shape[0], -1)\n",
    "labeled_users_idx = X_feat[\"user\"].isin(y_cat[\"user\"])\n",
    "labels[labeled_users_idx] = y_cat.set_index(\"user\").loc[X_feat[\"user\"][labeled_users_idx], \"anomtype\"].values\n",
    "\n",
    "# Fit Label Spreading\n",
    "label_model = LabelSpreading(kernel='knn', alpha=0.8, max_iter=1000)\n",
    "label_model.fit(X_train_scaled, labels)\n",
    "predicted_anomtype = label_model.transduction_\n",
    "\n",
    "# --- Step 3: Predict anomaly type for test users ---\n",
    "X_test_features = XX_feat[feature_cols]\n",
    "X_test_poly = poly.transform(X_test_features)\n",
    "X_test_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "# Predict cluster probabilities for test users using GMM\n",
    "test_probs = gmm.predict_proba(X_test_scaled)\n",
    "# Assign cluster based on max probability\n",
    "test_cluster_labels = np.argmax(test_probs, axis=1)\n",
    "# Map to anomaly type using cluster_to_class\n",
    "test_pred_class = [cluster_to_class.get(c, -1) for c in test_cluster_labels]\n",
    "\n",
    "# Use Label Spreading to refine test predictions\n",
    "test_pred_refined = label_model.predict(X_test_scaled)\n",
    "\n",
    "# Combine regression and refined classification for submission\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train_scaled, y[\"label\"])\n",
    "predicted_p_test = np.clip(ridge.predict(X_test_scaled), 0, 1)\n",
    "\n",
    "final_predictions = pd.DataFrame({\n",
    "    \"user\": XX_feat[\"user\"],\n",
    "    \"predicted_p\": predicted_p_test,\n",
    "    \"predicted_anomtype\": test_pred_refined  # refined by label spreading\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission_file = \"week1234_submission.csv\"\n",
    "final_predictions.to_csv(submission_file, index=False)\n",
    "print(f\"Submission saved to {submission_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085852f",
   "metadata": {},
   "source": [
    "# Qualitative analysis of each anomaly class type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1192ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   predicted_anomtype  count  min  max    median      mean       std  \\\n",
      "0                   0    372  0.0  1.0  0.519228  0.517127  0.278714   \n",
      "1                   1    337  0.0  1.0  0.394707  0.406708  0.241641   \n",
      "2                   2    191  0.0  1.0  0.682744  0.635228  0.261252   \n",
      "\n",
      "       skew  kurtosis  prop_low_0_2  prop_high_0_8  \n",
      "0 -0.001583 -1.099137      0.145161       0.217742  \n",
      "1  0.351972 -0.537550      0.228487       0.068249  \n",
      "2 -0.578649 -0.520357      0.073298       0.303665  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"week1234_submission.csv\")\n",
    "\n",
    "# Function to compute extra stats\n",
    "def prop_low(series, threshold=0.2):\n",
    "    return (series <= threshold).sum() / len(series)\n",
    "\n",
    "def prop_high(series, threshold=0.8):\n",
    "    return (series >= threshold).sum() / len(series)\n",
    "\n",
    "# Aggregate per anomaly type\n",
    "summary_table = df.groupby(\"predicted_anomtype\")[\"predicted_p\"].agg(\n",
    "    count='count',\n",
    "    min='min',\n",
    "    max='max',\n",
    "    median='median',\n",
    "    mean='mean',\n",
    "    std='std',\n",
    "    skew=skew,\n",
    "    kurtosis=kurtosis,\n",
    "    prop_low_0_2=prop_low,\n",
    "    prop_high_0_8=prop_high\n",
    ").reset_index()\n",
    "\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a38b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
